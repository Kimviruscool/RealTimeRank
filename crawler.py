import requests
from bs4 import BeautifulSoup
import time

# 1. ê³µí†µ í—¤ë” ì„¤ì • (ë¸Œë¼ìš°ì €ì¸ ì²™ ì†ì´ê¸° ìœ„í•¨)
# ë´‡ ì°¨ë‹¨ì„ ë§‰ê¸° ìœ„í•´ User-Agentë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì…ë‹ˆë‹¤.
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
}


def get_naver_news_headlines():
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ 'ì†ë³´(Breaking News)' ì„¹ì…˜ì˜ í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    url = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=001"  # ì†ë³´ í˜ì´ì§€
    data_list = []

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')

        # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì˜ì—­ ì„ íƒ (HTML êµ¬ì¡° ë¶„ì„ ê²°ê³¼)
        # headline ê¸°ì‚¬ì™€ ì¼ë°˜ ê¸°ì‚¬ê°€ ì„ì—¬ ìˆì–´ì„œ ë‘ ê·¸ë£¹ì„ ëª¨ë‘ ì°¾ìŠµë‹ˆë‹¤.
        articles = soup.select('.type06_headline li dl') + soup.select('.type06 li dl')

        print(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... ({len(articles)}ê°œ ë°œê²¬)")

        for article in articles:
            # a íƒœê·¸ ì°¾ê¸° (ì œëª©ê³¼ ë§í¬ê°€ ë“¤ì–´ìˆìŒ)
            link_tag = article.select_one('dt:not(.photo) a')  # ì‚¬ì§„ ì—†ëŠ” dt íƒœê·¸ ìš°ì„ 
            if link_tag is None:
                link_tag = article.select_one('dt.photo a')  # ì—†ìœ¼ë©´ ì‚¬ì§„ ìˆëŠ” íƒœê·¸

            if link_tag:
                title = link_tag.text.strip()
                link = link_tag['href']

                # ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if title:
                    data_list.append({'source': 'Naver News', 'title': title, 'link': link})

    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì—ëŸ¬: {e}")

    return data_list


def get_community_best():
    """
    ë””ì‹œì¸ì‚¬ì´ë“œ 'ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸' ê²Œì‹œíŒ ì œëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    (ì»¤ë®¤ë‹ˆí‹°ëŠ” HTML êµ¬ì¡°ê°€ ìì£¼ ë°”ë€Œë¯€ë¡œ ì£¼ì˜ í•„ìš”)
    """
    url = "https://gall.dcinside.com/board/lists/?id=dcbest&list_num=100&sort_type=N&search_head=1&page=1"
    data_list = []

    try:
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')

        # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ì„ íƒ (tr íƒœê·¸ ì¤‘ classê°€ ub-content ì¸ ê²ƒ)
        posts = soup.select('.gall_list .ub-content')

        print(f"âœ… ì»¤ë®¤ë‹ˆí‹° ìˆ˜ì§‘ ì¤‘... ({len(posts)}ê°œ ë°œê²¬)")

        for post in posts:
            title_tag = post.select_one('.gall_tit a')
            if title_tag:
                title = title_tag.text.strip()
                link = "https://gall.dcinside.com/board/lists/?id=dcbest&list_num=100&sort_type=N&search_head=1&page=1" + title_tag['href']

                if title:
                    data_list.append({'source': 'Community', 'title': title, 'link': link})

    except Exception as e:
        print(f"âŒ ì»¤ë®¤ë‹ˆí‹° í¬ë¡¤ë§ ì—ëŸ¬: {e}")

    return data_list


# --- ë©”ì¸ ì‹¤í–‰ë¶€ ---
if __name__ == "__main__":
    print("--- ğŸš€ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")

    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
    news_data = get_naver_news_headlines()

    # 2. ì»¤ë®¤ë‹ˆí‹° ìˆ˜ì§‘
    # (ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°)
    time.sleep(1)
    community_data = get_community_best()

    # 3. ê²°ê³¼ í•©ì¹˜ê¸°
    all_data = news_data + community_data

    print("\n--- ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ (ìƒìœ„ 10ê°œë§Œ ì¶œë ¥) ---")
    for idx, item in enumerate(all_data[:10], 1):
        print(f"[{idx}] [{item['source']}] {item['title']}")

    print(f"\nì´ {len(all_data)}ê°œì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")